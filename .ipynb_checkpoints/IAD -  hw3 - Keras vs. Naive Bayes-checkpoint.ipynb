{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №3 Классификация имен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Колос Мария ИАД-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def get_list(dir):\n",
    "    lines=[]\n",
    "    with open(dir, 'r') as f:\n",
    "        reader = csv.reader(f, dialect='excel', delimiter='\\n')\n",
    "        for row in reader:\n",
    "            lines.append(row[0])\n",
    "        return lines\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка данных. Удаляем неоднозначные имена (те, что присутствуют в обоих списках)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "female=get_list('hw3_data/female.txt')\n",
    "male=get_list('hw3_data/male.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2943, 5001)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(male),len(female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name in female:\n",
    "    if name in male:\n",
    "        female.remove(name)\n",
    "        male.remove(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2629, 4687)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(male),len(female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['X']=male+female\n",
    "df['Y']=[1 for i in range(len(male))]+[0 for j in range(len(female))]\n",
    "df['letter']=[row[0] for row in df.X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>letter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aamir</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abbot</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Abbott</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abby</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        X  Y letter\n",
       "0   Aamir  1      A\n",
       "1   Aaron  1      A\n",
       "2   Abbot  1      A\n",
       "3  Abbott  1      A\n",
       "4    Abby  1      A"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создаем тестовое множество по приципу - тестовая выборка - 20% от каждой буквы алфавита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train=[]\n",
    "test=[]\n",
    "for letter, words in df.groupby(['letter']):\n",
    "    train_,test_=train_test_split(words,test_size=0.2)\n",
    "    train.append(train_)\n",
    "    test.append(test_)\n",
    "test_df=pd.concat(test)\n",
    "train_df=pd.concat(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>letter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2664</th>\n",
       "      <td>Adria</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>Aili</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>Ashli</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>Alissa</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>Annabal</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            X  Y letter\n",
       "2664    Adria  0      A\n",
       "2700     Aili  0      A\n",
       "2994    Ashli  0      A\n",
       "2756   Alissa  0      A\n",
       "2887  Annabal  0      A"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проведем классификацию имен с помощью метода наивного Байеса. \n",
    "### В качестве признаков используем символьные n-граммы для n=2,3,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_ngrams(word, ng):\n",
    "    features=''\n",
    "    for ngram in ngrams(word,n=ng):\n",
    "        st=''\n",
    "        for letter in ngram:\n",
    "            st+=letter\n",
    "        features+=st+' '\n",
    "    return features.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True)\n",
    "test_df.reset_index(inplace=True)\n",
    "\n",
    "train_df.drop('index',1,inplace=True)\n",
    "test_df.drop('index',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        self.fit(X, y, **fit_params)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pip=Pipeline(\n",
    "[ ('vect',CountVectorizer()),\n",
    " ('tfidf',TfidfTransformer()),\n",
    " ('to_dense', DenseTransformer()),\n",
    " ('clf',GaussianNB())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  2\n",
      "accuracy =  0.705563093623\n",
      "F-score =  0.383522727273\n",
      "\n",
      "n =  3\n",
      "accuracy =  0.767978290366\n",
      "F-score =  0.597647058824\n",
      "\n",
      "n =  4\n",
      "accuracy =  0.774762550882\n",
      "F-score =  0.722408026756\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n=[2,3,4]\n",
    "\n",
    "for ng in n:\n",
    "    train=train_df['X'].apply(lambda x: merge_ngrams(x,ng))\n",
    "    test=test_df['X'].apply(lambda x: merge_ngrams(x,ng))\n",
    "\n",
    "    pip.fit(train,train_df['Y'])\n",
    "    print('n = ',ng)\n",
    "    print('accuracy = ',accuracy_score(test_df['Y'],pip.predict(test)))\n",
    "    print('F-score = ',f1_score(test_df['Y'],pip.predict(test)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наилучшие результаты были получены при n = 4.\n",
    "### Используем его в контрольном тесте для нахождения ошибок. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ally',\n",
       " 'Anni',\n",
       " 'Ara',\n",
       " 'Abner',\n",
       " 'Ahmad',\n",
       " 'Arther',\n",
       " 'Auberta',\n",
       " 'Abdel',\n",
       " 'Andrej',\n",
       " 'Ambros',\n",
       " 'Aldwin',\n",
       " 'Alyson',\n",
       " 'Alma',\n",
       " 'Ambrosio',\n",
       " 'Antoni',\n",
       " 'Annemarie',\n",
       " 'Audie',\n",
       " 'Arlo',\n",
       " 'Anurag',\n",
       " 'Alister',\n",
       " 'Ave',\n",
       " 'Allin',\n",
       " 'Arleta',\n",
       " 'Almira',\n",
       " 'Armand',\n",
       " 'Arvin',\n",
       " 'Aram',\n",
       " 'Averell',\n",
       " 'Andri',\n",
       " 'Averyl',\n",
       " 'Ainslie',\n",
       " 'Adam',\n",
       " 'Averill',\n",
       " 'Arthur',\n",
       " 'Ag',\n",
       " 'Adolphus',\n",
       " 'Aguinaldo',\n",
       " 'Adolf',\n",
       " 'Aldric',\n",
       " 'Ashish',\n",
       " 'Andrei',\n",
       " 'Andrzej',\n",
       " 'Almire',\n",
       " 'Anjela',\n",
       " 'Angus',\n",
       " 'Audi',\n",
       " 'Andromache',\n",
       " 'Aamir',\n",
       " 'Ashby',\n",
       " 'Alasdair',\n",
       " 'Auria',\n",
       " 'Abby',\n",
       " 'Alphonse',\n",
       " 'Alfonse',\n",
       " 'Adams',\n",
       " 'Agata',\n",
       " 'Aimil',\n",
       " 'Bess',\n",
       " 'Berk',\n",
       " 'Bernadine',\n",
       " 'Bianka',\n",
       " 'Bella',\n",
       " 'Bernd',\n",
       " 'Baillie',\n",
       " 'Brewer',\n",
       " 'Brendan',\n",
       " 'Brittney',\n",
       " 'Bendite',\n",
       " 'Bel',\n",
       " 'Bette-Ann',\n",
       " 'Benji',\n",
       " 'Bernetta',\n",
       " 'Butch',\n",
       " 'Burta',\n",
       " 'Brande',\n",
       " 'Brad',\n",
       " 'Binny',\n",
       " 'Brina',\n",
       " 'Becka',\n",
       " 'Brea',\n",
       " 'Blondie',\n",
       " 'Brock',\n",
       " 'Bailie',\n",
       " 'Bear',\n",
       " 'Becky',\n",
       " 'Bard',\n",
       " 'Blisse',\n",
       " 'Barry',\n",
       " 'Binky',\n",
       " 'Berry',\n",
       " 'Boris',\n",
       " 'Brena',\n",
       " 'Bethany',\n",
       " 'Brita',\n",
       " 'Biddy',\n",
       " 'Bubba',\n",
       " 'Blondelle',\n",
       " 'Becki',\n",
       " 'Bunnie',\n",
       " 'Betty',\n",
       " 'Bennett',\n",
       " 'Barnard',\n",
       " 'Bryan',\n",
       " 'Cordie',\n",
       " 'Cornelius',\n",
       " 'Celina',\n",
       " 'Claus',\n",
       " 'Celisse',\n",
       " 'Corabel',\n",
       " 'Christiano',\n",
       " 'Cora',\n",
       " 'Cristy',\n",
       " 'Cordelie',\n",
       " 'Coriss',\n",
       " 'Clarey',\n",
       " 'Clovis',\n",
       " 'Cornelia',\n",
       " 'Cyrille',\n",
       " 'Clayton',\n",
       " 'Caresse',\n",
       " 'Carmelia',\n",
       " 'Courtenay',\n",
       " 'Cristopher',\n",
       " 'Corbin',\n",
       " 'Chuck',\n",
       " 'Crissy',\n",
       " 'Chadd',\n",
       " 'Corette',\n",
       " 'Christos',\n",
       " 'Corabella',\n",
       " 'Christal',\n",
       " 'Carissa',\n",
       " 'Chevalier',\n",
       " 'Clementina',\n",
       " 'Clarita',\n",
       " 'Catherina',\n",
       " 'Candida',\n",
       " 'Carrol',\n",
       " 'Charyl',\n",
       " 'Celestine',\n",
       " 'Cletus',\n",
       " 'Carlo',\n",
       " 'Caro',\n",
       " 'Carli',\n",
       " 'Chevy',\n",
       " 'Cleland',\n",
       " 'Camellia',\n",
       " 'Cristine',\n",
       " 'Christen',\n",
       " 'Curtice',\n",
       " 'Davida',\n",
       " 'Douglis',\n",
       " 'Darell',\n",
       " 'Deanna',\n",
       " 'Debra',\n",
       " 'Dedra',\n",
       " 'Dinnie',\n",
       " 'Denise',\n",
       " 'Doti',\n",
       " 'Dwane',\n",
       " 'Danyelle',\n",
       " 'Delila',\n",
       " 'Dayna',\n",
       " 'Demetrius',\n",
       " 'Dominic',\n",
       " 'Dehlia',\n",
       " 'Deonne',\n",
       " 'Dorelle',\n",
       " 'Derek',\n",
       " 'Dimitrios',\n",
       " 'Dulce',\n",
       " 'Dino',\n",
       " 'Duke',\n",
       " 'Dickey',\n",
       " 'Debi',\n",
       " 'Diane',\n",
       " 'Damara',\n",
       " 'Dido',\n",
       " 'Duane',\n",
       " 'Derrol',\n",
       " 'Dyanna',\n",
       " 'Dmitri',\n",
       " 'Dahlia',\n",
       " 'Emmery',\n",
       " 'Eddy',\n",
       " 'Elane',\n",
       " 'Elmore',\n",
       " 'Eveleen',\n",
       " 'Erica',\n",
       " 'Erna',\n",
       " 'Elberta',\n",
       " 'Eba',\n",
       " 'Easton',\n",
       " 'Eloisa',\n",
       " 'Emlynne',\n",
       " 'Elvin',\n",
       " 'Engelbart',\n",
       " 'Estell',\n",
       " 'Edwin',\n",
       " 'Evey',\n",
       " 'Emerson',\n",
       " 'Eolanda',\n",
       " 'Edmund',\n",
       " 'Eliza',\n",
       " 'Elton',\n",
       " 'Elayne',\n",
       " 'Ellette',\n",
       " 'Evan',\n",
       " 'Essy',\n",
       " 'Eveline',\n",
       " 'Eric',\n",
       " 'Eyde',\n",
       " 'Eden',\n",
       " 'Erich',\n",
       " 'Emmeline',\n",
       " 'Elspeth',\n",
       " 'Earle',\n",
       " 'Ernest',\n",
       " 'Eugine',\n",
       " 'Fernandina',\n",
       " 'Fredericka',\n",
       " 'Fergus',\n",
       " 'Fitzgerald',\n",
       " 'Fayth',\n",
       " 'Faydra',\n",
       " 'Fowler',\n",
       " 'Florenza',\n",
       " 'Flora',\n",
       " 'Fiona',\n",
       " 'Francois',\n",
       " 'Flemming',\n",
       " 'Fredra',\n",
       " 'Ferne',\n",
       " 'Felipa',\n",
       " 'Foster',\n",
       " 'Farand',\n",
       " 'Flo',\n",
       " 'Frankie',\n",
       " 'Frederica',\n",
       " 'Gaby',\n",
       " 'Gwenora',\n",
       " 'Gavin',\n",
       " 'Geoffrey',\n",
       " 'Goldy',\n",
       " 'Gilles',\n",
       " 'Gerome',\n",
       " 'Godfree',\n",
       " 'Gav',\n",
       " 'Gerhardine',\n",
       " 'Giorgia',\n",
       " 'Gita',\n",
       " 'Gabie',\n",
       " 'Gianna',\n",
       " 'Grazia',\n",
       " 'Glenda',\n",
       " 'Griffin',\n",
       " 'Genny',\n",
       " 'Gasper',\n",
       " 'Gaynor',\n",
       " 'Galina',\n",
       " 'Garrot',\n",
       " 'Graehme',\n",
       " 'Gusella',\n",
       " 'Graig',\n",
       " 'Gabey',\n",
       " 'Goldia',\n",
       " 'Gusti',\n",
       " 'Gardener',\n",
       " 'Gregg',\n",
       " 'Gershon',\n",
       " 'Gilberto',\n",
       " 'Gibb',\n",
       " 'Gustie',\n",
       " 'Gabi',\n",
       " 'Goldie',\n",
       " 'Halvard',\n",
       " 'Hyacinth',\n",
       " 'Humbert',\n",
       " 'Honey',\n",
       " 'Harris',\n",
       " 'Hakeem',\n",
       " 'Hillel',\n",
       " 'Harvey',\n",
       " 'Hassan',\n",
       " 'Hilliard',\n",
       " 'Hanan',\n",
       " 'Hunter',\n",
       " 'Hagan',\n",
       " 'Hasty',\n",
       " 'Hope',\n",
       " 'Havivah',\n",
       " 'Harman',\n",
       " 'Herrick',\n",
       " 'Hugh',\n",
       " 'Helaina',\n",
       " 'Harv',\n",
       " 'Harvie',\n",
       " 'Hetti',\n",
       " 'Hoyt',\n",
       " 'Harrie',\n",
       " 'Hendrik',\n",
       " 'Herta',\n",
       " 'Hamlet',\n",
       " 'Hercule',\n",
       " 'Inci',\n",
       " 'Issi',\n",
       " 'Ignacio',\n",
       " 'Illa',\n",
       " 'Inga',\n",
       " 'Ichabod',\n",
       " 'Irina',\n",
       " 'Ilona',\n",
       " 'Jourdan',\n",
       " 'Job',\n",
       " 'Johnette',\n",
       " 'Jef',\n",
       " 'Jay',\n",
       " 'Josy',\n",
       " 'Joel',\n",
       " 'Jerrie',\n",
       " 'Joscelin',\n",
       " 'Jonis',\n",
       " 'Janos',\n",
       " 'Joya',\n",
       " 'Jonah',\n",
       " 'Jerrold',\n",
       " 'Jessey',\n",
       " 'Janice',\n",
       " 'Janeva',\n",
       " 'Jerrome',\n",
       " 'Jaine',\n",
       " 'Jarrett',\n",
       " 'Jocelin',\n",
       " 'Jenette',\n",
       " 'Jeffie',\n",
       " 'Jimbo',\n",
       " 'Joby',\n",
       " 'Jon',\n",
       " 'Jeralee',\n",
       " 'Jehanna',\n",
       " 'Joni',\n",
       " 'Jeniffer',\n",
       " 'Jessee',\n",
       " 'Jereme',\n",
       " 'Jessi',\n",
       " 'Jeremy',\n",
       " 'Jeromy',\n",
       " 'Jim',\n",
       " 'Kassie',\n",
       " 'Katleen',\n",
       " 'Kelvin',\n",
       " 'Koo',\n",
       " 'Krishna',\n",
       " 'Kassey',\n",
       " 'Kristal',\n",
       " 'Kali',\n",
       " 'Kippie',\n",
       " 'Krysta',\n",
       " 'Krissie',\n",
       " 'Keefe',\n",
       " 'Karlyn',\n",
       " 'Kimberley',\n",
       " 'Katusha',\n",
       " 'Karla',\n",
       " 'Kirbee',\n",
       " 'Kenny',\n",
       " 'Katey',\n",
       " 'Kaitlynn',\n",
       " 'Keith',\n",
       " 'Kellina',\n",
       " 'Kourtney',\n",
       " 'Kimmy',\n",
       " 'Klaus',\n",
       " 'Kattie',\n",
       " 'Kirstyn',\n",
       " 'Kally',\n",
       " 'Krystle',\n",
       " 'Kelcey',\n",
       " 'Lenore',\n",
       " 'Leonora',\n",
       " 'Lia',\n",
       " 'Libby',\n",
       " 'Latrena',\n",
       " 'Laurent',\n",
       " 'Lucienne',\n",
       " 'Lida',\n",
       " 'Leonid',\n",
       " 'Lenny',\n",
       " 'Lucas',\n",
       " 'Linell',\n",
       " 'Lesly',\n",
       " 'Lay',\n",
       " 'Luigi',\n",
       " 'Lillis',\n",
       " 'Loraine',\n",
       " 'Luise',\n",
       " 'Locke',\n",
       " 'Luna',\n",
       " 'Lucius',\n",
       " 'Leonelle',\n",
       " 'Lawson',\n",
       " 'Lisa',\n",
       " 'Lauri',\n",
       " 'Lyndon',\n",
       " 'Mart',\n",
       " 'Moss',\n",
       " 'May',\n",
       " 'Melvyn',\n",
       " 'Meggan',\n",
       " 'Marcellus',\n",
       " 'Marybeth',\n",
       " 'Marketa',\n",
       " 'Marcel',\n",
       " 'Merola',\n",
       " 'Mari',\n",
       " 'Mugsy',\n",
       " 'Marti',\n",
       " 'Mace',\n",
       " 'Marshall',\n",
       " 'Megan',\n",
       " 'Mame',\n",
       " 'Molli',\n",
       " 'Maria',\n",
       " 'Merilee',\n",
       " 'Merrel',\n",
       " 'Maurice',\n",
       " 'Mohamed',\n",
       " 'Morena',\n",
       " 'Mercedes',\n",
       " 'Mitchel',\n",
       " 'Moira',\n",
       " 'Melodee',\n",
       " 'Margaretha',\n",
       " 'Michell',\n",
       " 'Mandi',\n",
       " 'Malynda',\n",
       " 'Micaela',\n",
       " 'Marlie',\n",
       " 'Mei',\n",
       " 'Merl',\n",
       " 'Mureil',\n",
       " 'Megen',\n",
       " 'Myron',\n",
       " 'Morrie',\n",
       " 'Marigold',\n",
       " 'Marjory',\n",
       " 'Matthaeus',\n",
       " 'Marje',\n",
       " 'Murdoch',\n",
       " 'Mommy',\n",
       " 'Myke',\n",
       " 'Myrta',\n",
       " 'Marga',\n",
       " 'Magda',\n",
       " 'Mavra',\n",
       " 'Mortie',\n",
       " 'Mufinella',\n",
       " 'Matthew',\n",
       " 'Merlina',\n",
       " 'Marcela',\n",
       " 'Maxi',\n",
       " 'Marta',\n",
       " 'Marthe',\n",
       " 'Noland',\n",
       " 'Nil',\n",
       " 'Neel',\n",
       " 'Nessa',\n",
       " 'Nadean',\n",
       " 'Neda',\n",
       " 'Northrup',\n",
       " 'Noreen',\n",
       " 'Nelle',\n",
       " 'Norri',\n",
       " 'Nahum',\n",
       " 'Nickey',\n",
       " 'Nikki',\n",
       " 'Nikkie',\n",
       " 'Nissy',\n",
       " 'Nathanial',\n",
       " 'Nell',\n",
       " 'Osborne',\n",
       " 'Olle',\n",
       " 'Oral',\n",
       " 'Orsa',\n",
       " 'Orelia',\n",
       " 'Othello',\n",
       " 'Oran',\n",
       " 'Oralie',\n",
       " 'Odell',\n",
       " 'Osbourne',\n",
       " 'Owen',\n",
       " 'Oswell',\n",
       " 'Oralee',\n",
       " 'Pip',\n",
       " 'Piper',\n",
       " 'Portia',\n",
       " 'Peyton',\n",
       " 'Peta',\n",
       " 'Pepi',\n",
       " 'Patty',\n",
       " 'Pascal',\n",
       " 'Pail',\n",
       " 'Phillipe',\n",
       " 'Phillis',\n",
       " 'Prasad',\n",
       " 'Patricio',\n",
       " 'Prentice',\n",
       " 'Powell',\n",
       " 'Phylys',\n",
       " 'Penni',\n",
       " 'Patel',\n",
       " 'Pacifica',\n",
       " 'Pier',\n",
       " 'Paco',\n",
       " 'Quintilla',\n",
       " 'Quint',\n",
       " 'Rhoda',\n",
       " 'Rolf',\n",
       " 'Rycca',\n",
       " 'Reeba',\n",
       " 'Renaud',\n",
       " 'Roseline',\n",
       " 'Renard',\n",
       " 'Raynell',\n",
       " 'Roxanna',\n",
       " 'Rosanne',\n",
       " 'Reuven',\n",
       " 'Rufe',\n",
       " 'Randee',\n",
       " 'Rochella',\n",
       " 'Robbi',\n",
       " 'Risa',\n",
       " 'Reinhard',\n",
       " 'Robby',\n",
       " 'Renae',\n",
       " 'Romain',\n",
       " 'Rufus',\n",
       " 'Robinetta',\n",
       " 'Roth',\n",
       " 'Rem',\n",
       " 'Ritchie',\n",
       " 'Ransom',\n",
       " 'Randie',\n",
       " 'Robinett',\n",
       " 'Raf',\n",
       " 'Ricca',\n",
       " 'Rodge',\n",
       " 'Rod',\n",
       " 'Rutger',\n",
       " 'Rosella',\n",
       " 'Ralf',\n",
       " 'Ranee',\n",
       " 'Roarke',\n",
       " 'Remus',\n",
       " 'Salome',\n",
       " 'Staford',\n",
       " 'Siouxie',\n",
       " 'Solange',\n",
       " 'Say',\n",
       " 'Sig',\n",
       " 'Sollie',\n",
       " 'Sax',\n",
       " 'Stanleigh',\n",
       " 'Stig',\n",
       " 'Sherie',\n",
       " 'Shelton',\n",
       " 'Shannah',\n",
       " 'Sergent',\n",
       " 'Silvana',\n",
       " 'Sallyanne',\n",
       " 'Skipton',\n",
       " 'Sisely',\n",
       " 'Standford',\n",
       " 'Sanford',\n",
       " 'Sherrie',\n",
       " 'Sharron',\n",
       " 'Stephani',\n",
       " 'Shoshanna',\n",
       " 'Shumeet',\n",
       " 'Sayers',\n",
       " 'Sawyere',\n",
       " 'Shani',\n",
       " 'Stanton',\n",
       " 'Shannen',\n",
       " 'Sergeant',\n",
       " 'Stafford',\n",
       " 'Sherwood',\n",
       " 'Silvie',\n",
       " 'Spence',\n",
       " 'Sawyer',\n",
       " 'Sissy',\n",
       " 'Stephannie',\n",
       " 'Stace',\n",
       " 'Shem',\n",
       " 'Stanly',\n",
       " 'Sonja',\n",
       " 'Scot',\n",
       " 'Stew',\n",
       " 'Siward',\n",
       " 'Shep',\n",
       " 'Sid',\n",
       " 'Sybila',\n",
       " 'Sherwin',\n",
       " 'Sibbie',\n",
       " 'Secunda',\n",
       " 'Salomon',\n",
       " 'Todd',\n",
       " 'Tedman',\n",
       " 'Tova',\n",
       " 'Tammy',\n",
       " 'Tabbatha',\n",
       " 'Thornie',\n",
       " 'Theodore',\n",
       " 'Toddy',\n",
       " 'Tharen',\n",
       " 'Thorn',\n",
       " 'Thad',\n",
       " 'Trevar',\n",
       " 'Tucker',\n",
       " 'Thorndike',\n",
       " 'Thibaud',\n",
       " 'Tomiko',\n",
       " 'Tedd',\n",
       " 'Tiffi',\n",
       " 'Thaddus',\n",
       " 'Tiebold',\n",
       " 'Tyrone',\n",
       " 'Thornton',\n",
       " 'Tomlin',\n",
       " 'Thane',\n",
       " 'Tobye',\n",
       " 'Terrel',\n",
       " 'Taylor',\n",
       " 'Torie',\n",
       " 'Theadora',\n",
       " 'Tyrus',\n",
       " 'Thekla',\n",
       " 'Thaddius',\n",
       " 'Tess',\n",
       " 'Trista',\n",
       " 'Timi',\n",
       " 'Upton',\n",
       " 'Ulla',\n",
       " 'Uta',\n",
       " 'Ulysses',\n",
       " 'Vail',\n",
       " 'Veronica',\n",
       " 'Vince',\n",
       " 'Vanda',\n",
       " 'Vachel',\n",
       " 'Verla',\n",
       " 'Vena',\n",
       " 'Vanny',\n",
       " 'Vanna',\n",
       " 'Vickie',\n",
       " 'Valdemar',\n",
       " 'Wilbur',\n",
       " 'Wright',\n",
       " 'Ward',\n",
       " 'Wilow',\n",
       " 'Worth',\n",
       " 'Wiley',\n",
       " 'Wyatan',\n",
       " 'Wash',\n",
       " 'Walter',\n",
       " 'Whitman',\n",
       " 'Witold',\n",
       " 'Wit',\n",
       " 'William',\n",
       " 'Woodie',\n",
       " 'Windham',\n",
       " 'Wren',\n",
       " 'Wolfram',\n",
       " 'Ximenes',\n",
       " 'Xymenes',\n",
       " 'Yalonda',\n",
       " 'Yovonnda',\n",
       " 'Yigal',\n",
       " 'Zippy',\n",
       " 'Zoe',\n",
       " 'Zelig',\n",
       " 'Zilvia',\n",
       " 'Zach',\n",
       " 'Zak',\n",
       " 'Zeus',\n",
       " 'Zolly',\n",
       " 'Zorina']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 4\n",
    "train=train_df['X'].apply(lambda x: merge_ngrams(x,n))\n",
    "test=test_df['X'].apply(lambda x: merge_ngrams(x,n))\n",
    "\n",
    "pip.fit(train,train_df['Y'])\n",
    "\n",
    "\n",
    "predicted = pip.predict(test)\n",
    "\n",
    "failed_names = [(test_df['X'][error]) for error, yy in enumerate(zip(predicted, [row for row in train_df['Y']])) \n",
    "    if yy[0] != yy[1]\n",
    "]\n",
    "\n",
    "failed_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проанализируем окончания ошибочных имен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "endings=[x[-3:] for x in failed_names]\n",
    "\n",
    "end_freq=((key,freq )for (key,freq) in zip(Counter(endings).keys(),Counter(endings).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ina', 11)\n",
      "('ell', 10)\n",
      "('ine', 10)\n",
      "('ton', 9)\n",
      "('lla', 9)\n",
      "('ard', 8)\n",
      "('nna', 7)\n",
      "('lie', 7)\n",
      "('nda', 7)\n",
      "('rie', 7)\n",
      "('lle', 7)\n",
      "('lia', 6)\n",
      "('rta', 6)\n",
      "('die', 6)\n",
      "('nne', 5)\n",
      "('ora', 5)\n",
      "('nny', 5)\n",
      "('ane', 5)\n",
      "('ena', 4)\n",
      "('old', 4)\n",
      "('ord', 4)\n",
      "('ius', 4)\n",
      "('tte', 4)\n",
      "('ter', 4)\n",
      "('ssy', 4)\n",
      "('lin', 4)\n",
      "('ice', 4)\n",
      "('and', 4)\n",
      "('vin', 4)\n",
      "('ica', 4)\n",
      "('nie', 4)\n",
      "('dra', 3)\n",
      "('isa', 3)\n",
      "('eta', 3)\n",
      "('ore', 3)\n",
      "('een', 3)\n",
      "('win', 3)\n",
      "('lis', 3)\n",
      "('bby', 3)\n",
      "('ela', 3)\n",
      "('son', 3)\n",
      "('ome', 3)\n",
      "('tie', 3)\n",
      "('ita', 3)\n",
      "('gan', 3)\n",
      "('ida', 3)\n",
      "('ney', 3)\n",
      "('lly', 3)\n",
      "('sse', 3)\n",
      "('man', 3)\n",
      "('rne', 3)\n",
      "('ddy', 3)\n",
      "('ett', 3)\n",
      "('kie', 3)\n",
      "('mmy', 3)\n",
      "('lee', 3)\n",
      "('ley', 2)\n",
      "('ira', 2)\n",
      "('tha', 2)\n",
      "('nni', 2)\n",
      "('ila', 2)\n",
      "('ace', 2)\n",
      "('gus', 2)\n",
      "('sty', 2)\n",
      "('ani', 2)\n",
      "('sey', 2)\n",
      "('ail', 2)\n",
      "('per', 2)\n",
      "('sta', 2)\n",
      "('aud', 2)\n",
      "('ris', 2)\n",
      "('cca', 2)\n",
      "('rey', 2)\n",
      "('ent', 2)\n",
      "('eus', 2)\n",
      "('rol', 2)\n",
      "('ssi', 2)\n",
      "('bie', 2)\n",
      "('eth', 2)\n",
      "('ron', 2)\n",
      "('tta', 2)\n",
      "('art', 2)\n",
      "('dee', 2)\n",
      "('nse', 2)\n",
      "('ryl', 2)\n",
      "('ren', 2)\n",
      "('rel', 2)\n",
      "('rry', 2)\n",
      "('vey', 2)\n",
      "('ssa', 2)\n",
      "('ess', 2)\n",
      "('tal', 2)\n",
      "('ram', 2)\n",
      "('tty', 2)\n",
      "('ier', 2)\n",
      "('key', 2)\n",
      "('rlo', 2)\n",
      "('ona', 2)\n",
      "('ria', 2)\n",
      "('hel', 2)\n",
      "('her', 2)\n",
      "('ric', 2)\n",
      "('nah', 2)\n",
      "('sie', 2)\n",
      "('cio', 2)\n",
      "('olf', 2)\n",
      "('oni', 2)\n",
      "('aus', 2)\n",
      "('nce', 2)\n",
      "('vie', 2)\n",
      "('rla', 2)\n",
      "('ner', 2)\n",
      "('ise', 2)\n",
      "('cka', 2)\n",
      "('dan', 2)\n",
      "('nes', 2)\n",
      "('ood', 1)\n",
      "('eet', 1)\n",
      "('ule', 1)\n",
      "('rei', 1)\n",
      "('yne', 1)\n",
      "('tew', 1)\n",
      "('rle', 1)\n",
      "('cas', 1)\n",
      "('est', 1)\n",
      "('vah', 1)\n",
      "('ker', 1)\n",
      "('ery', 1)\n",
      "('hep', 1)\n",
      "('igh', 1)\n",
      "('ses', 1)\n",
      "('iza', 1)\n",
      "('nth', 1)\n",
      "('mir', 1)\n",
      "('imi', 1)\n",
      "('aby', 1)\n",
      "('Ara', 1)\n",
      "('med', 1)\n",
      "('tti', 1)\n",
      "('nos', 1)\n",
      "('del', 1)\n",
      "('eil', 1)\n",
      "('rej', 1)\n",
      "('Jay', 1)\n",
      "('sio', 1)\n",
      "('ffi', 1)\n",
      "('nja', 1)\n",
      "('ich', 1)\n",
      "('nde', 1)\n",
      "('evy', 1)\n",
      "('sti', 1)\n",
      "('tos', 1)\n",
      "('rek', 1)\n",
      "('Eba', 1)\n",
      "('ugh', 1)\n",
      "('vra', 1)\n",
      "('ata', 1)\n",
      "('fin', 1)\n",
      "('udi', 1)\n",
      "('epi', 1)\n",
      "('oss', 1)\n",
      "('ert', 1)\n",
      "('ear', 1)\n",
      "('don', 1)\n",
      "('vyn', 1)\n",
      "('hus', 1)\n",
      "('una', 1)\n",
      "('all', 1)\n",
      "('Zoe', 1)\n",
      "('erk', 1)\n",
      "('eba', 1)\n",
      "('ely', 1)\n",
      "('ufe', 1)\n",
      "('emy', 1)\n",
      "('gal', 1)\n",
      "('lma', 1)\n",
      "('ldo', 1)\n",
      "('rot', 1)\n",
      "('rli', 1)\n",
      "('nga', 1)\n",
      "('via', 1)\n",
      "('oby', 1)\n",
      "('Jef', 1)\n",
      "('the', 1)\n",
      "('ams', 1)\n",
      "('cot', 1)\n",
      "('Sid', 1)\n",
      "('lyn', 1)\n",
      "('lus', 1)\n",
      "('arv', 1)\n",
      "('nae', 1)\n",
      "('wen', 1)\n",
      "('mbo', 1)\n",
      "('Say', 1)\n",
      "('gsy', 1)\n",
      "('hum', 1)\n",
      "('ral', 1)\n",
      "('rje', 1)\n",
      "('vis', 1)\n",
      "('nic', 1)\n",
      "('lys', 1)\n",
      "('ish', 1)\n",
      "('uri', 1)\n",
      "('omy', 1)\n",
      "('any', 1)\n",
      "('och', 1)\n",
      "('osy', 1)\n",
      "('gia', 1)\n",
      "('yde', 1)\n",
      "('rik', 1)\n",
      "('tri', 1)\n",
      "('hur', 1)\n",
      "('Ann', 1)\n",
      "('int', 1)\n",
      "('mon', 1)\n",
      "('hby', 1)\n",
      "('Flo', 1)\n",
      "('bbi', 1)\n",
      "('kla', 1)\n",
      "('cki', 1)\n",
      "('ran', 1)\n",
      "('sad', 1)\n",
      "('sly', 1)\n",
      "('ger', 1)\n",
      "('eem', 1)\n",
      "('mil', 1)\n",
      "('lel', 1)\n",
      "('nid', 1)\n",
      "('rnd', 1)\n",
      "('mus', 1)\n",
      "('ean', 1)\n",
      "('hew', 1)\n",
      "('bod', 1)\n",
      "('ers', 1)\n",
      "('nly', 1)\n",
      "('ros', 1)\n",
      "('uke', 1)\n",
      "('iko', 1)\n",
      "('iss', 1)\n",
      "('alf', 1)\n",
      "('dia', 1)\n",
      "('dus', 1)\n",
      "('Sig', 1)\n",
      "('egg', 1)\n",
      "('ndi', 1)\n",
      "('ree', 1)\n",
      "('ere', 1)\n",
      "('rad', 1)\n",
      "('tey', 1)\n",
      "('nay', 1)\n",
      "('gda', 1)\n",
      "('ipe', 1)\n",
      "('let', 1)\n",
      "('cke', 1)\n",
      "('nge', 1)\n",
      "('ash', 1)\n",
      "('ham', 1)\n",
      "('iam', 1)\n",
      "('see', 1)\n",
      "('che', 1)\n",
      "('nan', 1)\n",
      "('ght', 1)\n",
      "('Job', 1)\n",
      "('bin', 1)\n",
      "('cky', 1)\n",
      "('hna', 1)\n",
      "('Zak', 1)\n",
      "('axi', 1)\n",
      "('Lia', 1)\n",
      "('rsa', 1)\n",
      "('san', 1)\n",
      "('yan', 1)\n",
      "('oyt', 1)\n",
      "('oda', 1)\n",
      "('eva', 1)\n",
      "('cey', 1)\n",
      "('add', 1)\n",
      "('odd', 1)\n",
      "('ppy', 1)\n",
      "('tch', 1)\n",
      "('lig', 1)\n",
      "('bye', 1)\n",
      "('ock', 1)\n",
      "('rag', 1)\n",
      "('one', 1)\n",
      "('yna', 1)\n",
      "('bur', 1)\n",
      "('tia', 1)\n",
      "('Jim', 1)\n",
      "('efe', 1)\n",
      "('hem', 1)\n",
      "('ynn', 1)\n",
      "('fie', 1)\n",
      "('fus', 1)\n",
      "('zej', 1)\n",
      "('rea', 1)\n",
      "('ler', 1)\n",
      "('Raf', 1)\n",
      "('cal', 1)\n",
      "('ola', 1)\n",
      "('tig', 1)\n",
      "('Pip', 1)\n",
      "('hon', 1)\n",
      "('dge', 1)\n",
      "('nen', 1)\n",
      "('ven', 1)\n",
      "('bra', 1)\n",
      "('hie', 1)\n",
      "('ova', 1)\n",
      "('bba', 1)\n",
      "('kki', 1)\n",
      "('uck', 1)\n",
      "('ibb', 1)\n",
      "('Mei', 1)\n",
      "('des', 1)\n",
      "('tus', 1)\n",
      "('nky', 1)\n",
      "('Ag', 1)\n",
      "('sha', 1)\n",
      "('lli', 1)\n",
      "('yke', 1)\n",
      "('ant', 1)\n",
      "('den', 1)\n",
      "('ing', 1)\n",
      "('pie', 1)\n",
      "('bel', 1)\n",
      "('nji', 1)\n",
      "('gen', 1)\n",
      "('und', 1)\n",
      "('ire', 1)\n",
      "('rti', 1)\n",
      "('bey', 1)\n",
      "('ain', 1)\n",
      "('yth', 1)\n",
      "('low', 1)\n",
      "('nee', 1)\n",
      "('Sax', 1)\n",
      "('rke', 1)\n",
      "('ial', 1)\n",
      "('ith', 1)\n",
      "('ois', 1)\n",
      "('Rod', 1)\n",
      "('nor', 1)\n",
      "('Rem', 1)\n",
      "('Gav', 1)\n",
      "('ari', 1)\n",
      "('Lay', 1)\n",
      "('Nil', 1)\n",
      "('nka', 1)\n",
      "('Jon', 1)\n",
      "('les', 1)\n",
      "('yer', 1)\n",
      "('erl', 1)\n",
      "('ios', 1)\n",
      "('nza', 1)\n",
      "('eda', 1)\n",
      "('zia', 1)\n",
      "('wer', 1)\n",
      "('mar', 1)\n",
      "('ike', 1)\n",
      "('air', 1)\n",
      "('ite', 1)\n",
      "('som', 1)\n",
      "('oth', 1)\n",
      "('Wit', 1)\n",
      "('ldy', 1)\n",
      "('aro', 1)\n",
      "('had', 1)\n",
      "('oya', 1)\n",
      "('ebi', 1)\n",
      "('igi', 1)\n",
      "('rto', 1)\n",
      "('rga', 1)\n",
      "('nci', 1)\n",
      "('rna', 1)\n",
      "('rri', 1)\n",
      "('tle', 1)\n",
      "('hme', 1)\n",
      "('ame', 1)\n",
      "('var', 1)\n",
      "('eel', 1)\n",
      "('bee', 1)\n",
      "('orn', 1)\n",
      "('ara', 1)\n",
      "('rus', 1)\n",
      "('oti', 1)\n",
      "('May', 1)\n",
      "('rth', 1)\n",
      "('ope', 1)\n",
      "('tel', 1)\n",
      "('Bel', 1)\n",
      "('ach', 1)\n",
      "('fer', 1)\n",
      "('llo', 1)\n",
      "('cel', 1)\n",
      "('tyn', 1)\n",
      "('dri', 1)\n",
      "('oel', 1)\n",
      "('ipa', 1)\n",
      "('van', 1)\n",
      "('Koo', 1)\n",
      "('ory', 1)\n",
      "('ido', 1)\n",
      "('ten', 1)\n",
      "('mad', 1)\n",
      "('Ave', 1)\n",
      "('nis', 1)\n",
      "('lor', 1)\n",
      "('edd', 1)\n",
      "('ino', 1)\n",
      "('Uta', 1)\n",
      "('eme', 1)\n",
      "('abi', 1)\n",
      "('ald', 1)\n",
      "('xie', 1)\n",
      "('ano', 1)\n",
      "('ick', 1)\n",
      "('lce', 1)\n",
      "('rup', 1)\n",
      "('tan', 1)\n",
      "('ana', 1)\n",
      "('ill', 1)\n",
      "('aig', 1)\n",
      "('aco', 1)\n",
      "('ali', 1)\n",
      "('dam', 1)\n"
     ]
    }
   ],
   "source": [
    "end_freq=sorted(end_freq, key=lambda tup: tup[1],reverse=True)\n",
    "\n",
    "for i in end_freq:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чаще всего классификатор ошибался на именах с окончаниями 'ina','ell','ine','ton','lla','ard'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используем сеть с двумя слоями LSTM для определения пола.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chars = set(  \"\".join(male) + \"\".join(female))#все встречающиеся в именах символы\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))#индексы для матрицы\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "maxlen = len(max( male , key=len)) + len(max( female , key=len))\n",
    "\n",
    "def get_letter_matrix(words):\n",
    "    matrix= np.zeros((len(words) ,maxlen, len(chars) ))\n",
    "    #print(matrix)\n",
    "    for i, name in enumerate(words):\n",
    "        for t, char in enumerate(name.lower()):\n",
    "            matrix[i, t,char_indices[char]] = 1\n",
    "            \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_y(sample_y):#вектор Y\n",
    "    y=np.zeros((len(sample_y) , 2 ))\n",
    "    for i in range(len(sample_y)):\n",
    "        y[i,sample_y[i]]=1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_input=get_letter_matrix(train_df['X'])\n",
    "x_test_input=get_letter_matrix(test_df['X'])\n",
    "\n",
    "y_train = get_y(train_df['Y'])\n",
    "y_test = get_y(test_df['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(nodes,activation):\n",
    "    nEpochs = 10\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(nodes, return_sequences=True, input_shape=(maxlen, len(chars))))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(nodes, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_y_pred(pred):\n",
    "    l=len(pred)\n",
    "    y_pred=[]\n",
    "    for j in range(ln):\n",
    "        y_pred.append(int(pred[j,1]>=0.5))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5842, 1474)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df),len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В случае наших объемов данных используем сеть с количеством узлов 256. Для лучшего результата подберем функцию активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5842/5842 [==============================] - 41s - loss: 0.5940    \n",
      "Epoch 2/10\n",
      "5842/5842 [==============================] - 37s - loss: 0.5062    \n",
      "Epoch 3/10\n",
      "5842/5842 [==============================] - 35s - loss: 0.4657    \n",
      "Epoch 4/10\n",
      "5842/5842 [==============================] - 35s - loss: 0.4368    \n",
      "Epoch 5/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.4214    \n",
      "Epoch 6/10\n",
      "5842/5842 [==============================] - 41s - loss: 0.4002    \n",
      "Epoch 7/10\n",
      "5842/5842 [==============================] - 41s - loss: 0.3810    \n",
      "Epoch 8/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.3599    \n",
      "Epoch 9/10\n",
      "5842/5842 [==============================] - 41s - loss: 0.3502    \n",
      "Epoch 10/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.3444    \n",
      "===activation:  softmax\n",
      "accuracy:  0.823609226594\n",
      "F-score:  0.782608695652\n",
      "Epoch 1/10\n",
      "5842/5842 [==============================] - 45s - loss: 0.6781    \n",
      "Epoch 2/10\n",
      "5842/5842 [==============================] - 42s - loss: 0.6009    \n",
      "Epoch 3/10\n",
      "5842/5842 [==============================] - 42s - loss: 0.5973    \n",
      "Epoch 4/10\n",
      "5842/5842 [==============================] - 41s - loss: 0.5684    \n",
      "Epoch 5/10\n",
      "5842/5842 [==============================] - 41s - loss: 0.5355    \n",
      "Epoch 6/10\n",
      "5842/5842 [==============================] - 37s - loss: 0.5739    \n",
      "Epoch 7/10\n",
      "5842/5842 [==============================] - 36s - loss: 0.5510    \n",
      "Epoch 8/10\n",
      "5842/5842 [==============================] - 36s - loss: 0.4856    \n",
      "Epoch 9/10\n",
      "5842/5842 [==============================] - 36s - loss: 0.5186    \n",
      "Epoch 10/10\n",
      "5842/5842 [==============================] - 36s - loss: 0.4403    \n",
      "===activation:  softplus\n",
      "accuracy:  0.785617367707\n",
      "F-score:  0.752351097179\n",
      "Epoch 1/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.7614    \n",
      "Epoch 2/10\n",
      "5842/5842 [==============================] - 36s - loss: 0.8370    \n",
      "Epoch 3/10\n",
      "5842/5842 [==============================] - 36s - loss: 0.8082    \n",
      "Epoch 4/10\n",
      "5842/5842 [==============================] - 36s - loss: 0.7421    \n",
      "Epoch 5/10\n",
      "5842/5842 [==============================] - 36s - loss: 0.8323    \n",
      "Epoch 6/10\n",
      "5842/5842 [==============================] - 36s - loss: 0.8414    \n",
      "Epoch 7/10\n",
      "5842/5842 [==============================] - 36s - loss: 0.9304    \n",
      "Epoch 8/10\n",
      "5842/5842 [==============================] - 37s - loss: 0.9992    \n",
      "Epoch 9/10\n",
      "5842/5842 [==============================] - 37s - loss: 0.9024    \n",
      "Epoch 10/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.9973    \n",
      "===activation:  softsign\n",
      "accuracy:  0.649932157395\n",
      "F-score:  0.634042553191\n",
      "Epoch 1/10\n",
      "5842/5842 [==============================] - 43s - loss: 0.7670    \n",
      "Epoch 2/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.7402    \n",
      "Epoch 3/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.6876    \n",
      "Epoch 4/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.6350    \n",
      "Epoch 5/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.6361    \n",
      "Epoch 6/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.5779    \n",
      "Epoch 7/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.6338    \n",
      "Epoch 8/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.5653    \n",
      "Epoch 9/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.7222    \n",
      "Epoch 10/10\n",
      "5842/5842 [==============================] - 37s - loss: 2.1257    \n",
      "===activation:  relu\n",
      "accuracy:  0.735413839891\n",
      "F-score:  0.668930390492\n",
      "Epoch 1/10\n",
      "5842/5842 [==============================] - 40s - loss: 0.7431    \n",
      "Epoch 2/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.6409    \n",
      "Epoch 3/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.6206    \n",
      "Epoch 4/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.5706    \n",
      "Epoch 5/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.5695    \n",
      "Epoch 6/10\n",
      "5842/5842 [==============================] - 38s - loss: 0.5410    \n",
      "Epoch 7/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.6187    \n",
      "Epoch 8/10\n",
      "5842/5842 [==============================] - 44s - loss: 0.5198    \n",
      "Epoch 9/10\n",
      "5842/5842 [==============================] - 45s - loss: 0.4639    \n",
      "Epoch 10/10\n",
      "5842/5842 [==============================] - 46s - loss: 0.4836    \n",
      "===activation:  tanh\n",
      "accuracy:  0.573270013569\n",
      "F-score:  0.621767889357\n",
      "Epoch 1/10\n",
      "5842/5842 [==============================] - 46s - loss: 0.5760    \n",
      "Epoch 2/10\n",
      "5842/5842 [==============================] - 42s - loss: 0.5057    \n",
      "Epoch 3/10\n",
      "5842/5842 [==============================] - 43s - loss: 0.4653    \n",
      "Epoch 4/10\n",
      "5842/5842 [==============================] - 43s - loss: 0.4340    \n",
      "Epoch 5/10\n",
      "5842/5842 [==============================] - 43s - loss: 0.4181    \n",
      "Epoch 6/10\n",
      "5842/5842 [==============================] - 43s - loss: 0.3949    \n",
      "Epoch 7/10\n",
      "5842/5842 [==============================] - 42s - loss: 0.3781    \n",
      "Epoch 8/10\n",
      "5842/5842 [==============================] - 43s - loss: 0.3725    \n",
      "Epoch 9/10\n",
      "5842/5842 [==============================] - 45s - loss: 0.3560    \n",
      "Epoch 10/10\n",
      "5842/5842 [==============================] - 43s - loss: 0.3411    \n",
      "===activation:  sigmoid\n",
      "accuracy:  0.841248303935\n",
      "F-score:  0.787272727273\n",
      "Epoch 1/10\n",
      "5842/5842 [==============================] - 46s - loss: 0.5903    \n",
      "Epoch 2/10\n",
      "5842/5842 [==============================] - 46s - loss: 0.5711    \n",
      "Epoch 3/10\n",
      "5842/5842 [==============================] - 44s - loss: 0.5379    \n",
      "Epoch 4/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.5409    \n",
      "Epoch 5/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.4984    \n",
      "Epoch 6/10\n",
      "5842/5842 [==============================] - 41s - loss: 0.5295    \n",
      "Epoch 7/10\n",
      "5842/5842 [==============================] - 43s - loss: 0.6079    \n",
      "Epoch 8/10\n",
      "5842/5842 [==============================] - 40s - loss: 0.5289    \n",
      "Epoch 9/10\n",
      "5842/5842 [==============================] - 41s - loss: 0.5392    \n",
      "Epoch 10/10\n",
      "5842/5842 [==============================] - 44s - loss: 0.5140    \n",
      "===activation:  hard_sigmoid\n",
      "accuracy:  0.794436906377\n",
      "F-score:  0.674543501611\n",
      "Epoch 1/10\n",
      "5842/5842 [==============================] - 51s - loss: 0.8306    \n",
      "Epoch 2/10\n",
      "5842/5842 [==============================] - 46s - loss: 0.6633    \n",
      "Epoch 3/10\n",
      "5842/5842 [==============================] - 45s - loss: 0.6341    \n",
      "Epoch 4/10\n",
      "5842/5842 [==============================] - 42s - loss: 0.6115    \n",
      "Epoch 5/10\n",
      "5842/5842 [==============================] - 44s - loss: 0.6070    \n",
      "Epoch 6/10\n",
      "5842/5842 [==============================] - 43s - loss: 0.5295    \n",
      "Epoch 7/10\n",
      "5842/5842 [==============================] - 46s - loss: 0.5348    \n",
      "Epoch 8/10\n",
      "5842/5842 [==============================] - 46s - loss: 0.6335    \n",
      "Epoch 9/10\n",
      "5842/5842 [==============================] - 44s - loss: 0.5476    \n",
      "Epoch 10/10\n",
      "5842/5842 [==============================] - 41s - loss: 0.5530    \n",
      "===activation:  linear\n",
      "accuracy:  0.827001356852\n",
      "F-score:  0.752667313288\n"
     ]
    }
   ],
   "source": [
    "activations=['softmax','softplus','softsign','relu','tanh','sigmoid','hard_sigmoid','linear']\n",
    "for act in activations:\n",
    "    NN_model=build_model(256,act)\n",
    "    NN_model.fit(x_train_input, y_train, batch_size=16, nb_epoch=nEpochs)\n",
    "    predicted=NN_model.predict(x_test_input)\n",
    "    \n",
    "    y_pred=convert_y_pred(predicted)\n",
    "    \n",
    "    print('===activation: ',act)\n",
    "    print('accuracy: ',accuracy_score(test_df.Y,y_pred))\n",
    "    print('F-score: ',f1_score(test_df.Y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Самые высокие значения метрик были получены с использованием сигмоида.\n",
    "#### accuracy:  0.841248303935\n",
    "#### F-score:  0.787272727273"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используем его для получения ошибочных имен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5842/5842 [==============================] - 42s - loss: 0.5773    \n",
      "Epoch 2/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.5029    \n",
      "Epoch 3/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.4661    \n",
      "Epoch 4/10\n",
      "5842/5842 [==============================] - 40s - loss: 0.4356    \n",
      "Epoch 5/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.4140    \n",
      "Epoch 6/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.4009    \n",
      "Epoch 7/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.3750    \n",
      "Epoch 8/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.3556    \n",
      "Epoch 9/10\n",
      "5842/5842 [==============================] - 39s - loss: 0.3481    \n",
      "Epoch 10/10\n",
      "5842/5842 [==============================] - 40s - loss: 0.3367    \n"
     ]
    }
   ],
   "source": [
    "model=build_model(256,'sigmoid')\n",
    "model.fit(x_train_input, y_train, batch_size=16, nb_epoch=nEpochs)\n",
    "predicted=model.predict(x_test_input)\n",
    "    \n",
    "y_pred=convert_y_pred(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arther',\n",
       " 'Ashely',\n",
       " 'Andrej',\n",
       " 'Ardyth',\n",
       " 'Ame',\n",
       " 'Aldwin',\n",
       " 'Alyson',\n",
       " 'Antoni',\n",
       " 'Alisun',\n",
       " 'Anurag',\n",
       " 'Anthe',\n",
       " 'Andri',\n",
       " 'Averyl',\n",
       " 'Aub',\n",
       " 'Aldric',\n",
       " 'Ashish',\n",
       " 'Ambrosi',\n",
       " 'Anastasie',\n",
       " 'Andromache',\n",
       " 'Ardelle',\n",
       " 'Aamir',\n",
       " 'Ashby',\n",
       " 'Anett',\n",
       " 'Allsun',\n",
       " 'Aimil',\n",
       " 'Annalisa',\n",
       " 'Billy',\n",
       " 'Bess',\n",
       " 'Bjorne',\n",
       " 'Buddy',\n",
       " 'Beatriz',\n",
       " 'Baillie',\n",
       " 'Bel',\n",
       " 'Benji',\n",
       " 'Brad',\n",
       " 'Benjamen',\n",
       " 'Basil',\n",
       " 'Bailie',\n",
       " 'Benjie',\n",
       " 'Barry',\n",
       " 'Boris',\n",
       " 'Bethany',\n",
       " 'Blanch',\n",
       " 'Blaine',\n",
       " 'Bubba',\n",
       " 'Bell',\n",
       " 'Beilul',\n",
       " 'Bryan',\n",
       " 'Cain',\n",
       " 'Clarke',\n",
       " 'Christiano',\n",
       " 'Cathe',\n",
       " 'Cyrille',\n",
       " 'Carmon',\n",
       " 'Corbin',\n",
       " 'Cobby',\n",
       " 'Cole',\n",
       " 'Chadd',\n",
       " 'Chen',\n",
       " 'Carrol',\n",
       " 'Conway',\n",
       " 'Charlie',\n",
       " 'Chevy',\n",
       " 'Catlin',\n",
       " 'Conan',\n",
       " 'Cathrin',\n",
       " 'Darleen',\n",
       " 'Darell',\n",
       " 'Dwane',\n",
       " 'Dominic',\n",
       " 'Derek',\n",
       " 'Dulce',\n",
       " 'Duke',\n",
       " 'Duane',\n",
       " 'Dmitri',\n",
       " 'Emmery',\n",
       " 'Eddy',\n",
       " 'Elmore',\n",
       " 'Erinn',\n",
       " 'Estell',\n",
       " 'Evey',\n",
       " 'Eden',\n",
       " 'Etienne',\n",
       " 'Elspeth',\n",
       " 'Flynn',\n",
       " 'Fowler',\n",
       " 'Fern',\n",
       " 'Felipe',\n",
       " 'Francois',\n",
       " 'Flemming',\n",
       " 'Ferinand',\n",
       " 'Farand',\n",
       " 'Gerrit',\n",
       " 'Gerome',\n",
       " 'Godfree',\n",
       " 'Gredel',\n",
       " 'Gates',\n",
       " 'Griffin',\n",
       " 'Guy',\n",
       " 'Graig',\n",
       " 'Gibb',\n",
       " 'Helge',\n",
       " 'Hailey',\n",
       " 'Hyacinth',\n",
       " 'Herb',\n",
       " 'Herby',\n",
       " 'Hillel',\n",
       " 'Hari',\n",
       " 'Hagan',\n",
       " 'Hope',\n",
       " 'Harriott',\n",
       " 'Herve',\n",
       " 'Harvie',\n",
       " 'Hendrik',\n",
       " 'Iggy',\n",
       " 'Ingeborg',\n",
       " 'Isaak',\n",
       " 'Ines',\n",
       " 'Jourdan',\n",
       " 'Jonathan',\n",
       " 'Jay',\n",
       " 'Joel',\n",
       " 'Jerrie',\n",
       " 'Jamey',\n",
       " 'Jonah',\n",
       " 'Jessey',\n",
       " 'Jarrett',\n",
       " 'Jacquelin',\n",
       " 'Jon',\n",
       " 'Jessee',\n",
       " 'Jereme',\n",
       " 'Jorge',\n",
       " 'Joshua',\n",
       " 'Jim',\n",
       " 'Krishna',\n",
       " 'Kermie',\n",
       " 'Kimberley',\n",
       " 'Karsten',\n",
       " 'Kirk',\n",
       " 'Kenny',\n",
       " 'Keith',\n",
       " 'Kaleb',\n",
       " 'Konrad',\n",
       " 'Klaus',\n",
       " 'Karl',\n",
       " 'Lonny',\n",
       " 'Leonid',\n",
       " 'Lenny',\n",
       " 'Linell',\n",
       " 'Lay',\n",
       " 'Luigi',\n",
       " 'Lillis',\n",
       " 'Lindsey',\n",
       " 'Locke',\n",
       " 'Laney',\n",
       " 'Marylou',\n",
       " 'Mylo',\n",
       " 'Melvyn',\n",
       " 'Mugsy',\n",
       " 'Mace',\n",
       " 'Murray',\n",
       " 'Margot',\n",
       " 'Margalo',\n",
       " 'Merrel',\n",
       " 'Maurice',\n",
       " 'Mercedes',\n",
       " 'Mustafa',\n",
       " 'Michell',\n",
       " 'Mair',\n",
       " 'Mureil',\n",
       " 'Myron',\n",
       " 'Morrie',\n",
       " 'Marigold',\n",
       " 'Myke',\n",
       " 'Mortie',\n",
       " 'Margo',\n",
       " 'Madelon',\n",
       " 'Mabel',\n",
       " 'Nicol',\n",
       " 'Nil',\n",
       " 'Norman',\n",
       " 'Neel',\n",
       " 'Natty',\n",
       " 'Nikki',\n",
       " 'Nikkie',\n",
       " 'Nell',\n",
       " 'Osborne',\n",
       " 'Olle',\n",
       " 'Oral',\n",
       " 'Oran',\n",
       " 'Osbourne',\n",
       " 'Pip',\n",
       " 'Piper',\n",
       " 'Pail',\n",
       " 'Padraig',\n",
       " 'Penelope',\n",
       " 'Phillipe',\n",
       " 'Perl',\n",
       " 'Partha',\n",
       " 'Pier',\n",
       " 'Paco',\n",
       " 'Rayner',\n",
       " 'Reid',\n",
       " 'Rosamund',\n",
       " 'Roman',\n",
       " 'Rose',\n",
       " 'Romain',\n",
       " 'Rowe',\n",
       " 'Roth',\n",
       " 'Ronen',\n",
       " 'Rem',\n",
       " 'Rachele',\n",
       " 'Raf',\n",
       " 'Rad',\n",
       " 'Rod',\n",
       " 'Say',\n",
       " 'Sig',\n",
       " 'Sollie',\n",
       " 'Sheridan',\n",
       " 'Solly',\n",
       " 'Sharron',\n",
       " 'Sloane',\n",
       " 'Shamit',\n",
       " 'Sawyere',\n",
       " 'Spence',\n",
       " 'Salomone',\n",
       " 'Stace',\n",
       " 'Stanly',\n",
       " 'Sid',\n",
       " 'Schroeder',\n",
       " 'Thor',\n",
       " 'Tabbatha',\n",
       " 'Terese',\n",
       " 'Thornie',\n",
       " 'Theodore',\n",
       " 'Toddy',\n",
       " 'Tabatha',\n",
       " 'Tatum',\n",
       " 'Thaine',\n",
       " 'Thad',\n",
       " 'Tomiko',\n",
       " 'Tedd',\n",
       " 'Tore',\n",
       " 'Tyrone',\n",
       " 'Tomlin',\n",
       " 'Tess',\n",
       " 'Vail',\n",
       " 'Vachel',\n",
       " 'Venus',\n",
       " 'Wilow',\n",
       " 'Wayne',\n",
       " 'Weslie',\n",
       " 'Welby',\n",
       " 'Weylin',\n",
       " 'William',\n",
       " 'Woodie',\n",
       " 'Windham',\n",
       " 'Yehudi',\n",
       " 'Yigal',\n",
       " 'Yankee',\n",
       " 'Zippy',\n",
       " 'Zebedee',\n",
       " 'Zane',\n",
       " 'Zolly']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_names = [test_df.X[index]for index, pair in enumerate(zip(y_pred, test_df.Y)) if pair[0] != pair[1]]\n",
    "failed_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ell', 6)\n",
      "('lie', 5)\n",
      "('lin', 4)\n",
      "('ane', 4)\n",
      "('tha', 3)\n",
      "('ore', 3)\n",
      "('lly', 3)\n",
      "('nny', 3)\n",
      "('rne', 3)\n",
      "('ddy', 3)\n",
      "('lle', 3)\n",
      "('ley', 2)\n",
      "('sey', 2)\n",
      "('ope', 2)\n",
      "('one', 2)\n",
      "('ail', 2)\n",
      "('man', 2)\n",
      "('ess', 2)\n",
      "('ace', 2)\n",
      "('and', 2)\n",
      "('the', 2)\n",
      "('sun', 2)\n",
      "('rie', 2)\n",
      "('ett', 2)\n",
      "('ine', 2)\n",
      "('ron', 2)\n",
      "('rad', 2)\n",
      "('ipe', 2)\n",
      "('dan', 2)\n",
      "('ain', 2)\n",
      "('aig', 2)\n",
      "('hel', 1)\n",
      "('ryl', 1)\n",
      "('nen', 1)\n",
      "('alo', 1)\n",
      "('rag', 1)\n",
      "('nne', 1)\n",
      "('got', 1)\n",
      "('lul', 1)\n",
      "('yne', 1)\n",
      "('isa', 1)\n",
      "('inn', 1)\n",
      "('ose', 1)\n",
      "('per', 1)\n",
      "('han', 1)\n",
      "('ois', 1)\n",
      "('ynn', 1)\n",
      "('Guy', 1)\n",
      "('ery', 1)\n",
      "('rej', 1)\n",
      "('sil', 1)\n",
      "('ler', 1)\n",
      "('nth', 1)\n",
      "('bby', 1)\n",
      "('gal', 1)\n",
      "('mir', 1)\n",
      "('rel', 1)\n",
      "('aco', 1)\n",
      "('aak', 1)\n",
      "('Pip', 1)\n",
      "('ham', 1)\n",
      "('osi', 1)\n",
      "('col', 1)\n",
      "('del', 1)\n",
      "('ier', 1)\n",
      "('eil', 1)\n",
      "('rry', 1)\n",
      "('Jay', 1)\n",
      "('bba', 1)\n",
      "('ran', 1)\n",
      "('vey', 1)\n",
      "('ibb', 1)\n",
      "('eel', 1)\n",
      "('des', 1)\n",
      "('ney', 1)\n",
      "('evy', 1)\n",
      "('hua', 1)\n",
      "('ari', 1)\n",
      "('ice', 1)\n",
      "('den', 1)\n",
      "('Ame', 1)\n",
      "('ing', 1)\n",
      "('hen', 1)\n",
      "('fin', 1)\n",
      "('udi', 1)\n",
      "('bel', 1)\n",
      "('riz', 1)\n",
      "('vyn', 1)\n",
      "('eth', 1)\n",
      "('rin', 1)\n",
      "('ere', 1)\n",
      "('ese', 1)\n",
      "('nes', 1)\n",
      "('Jim', 1)\n",
      "('tty', 1)\n",
      "('und', 1)\n",
      "('nie', 1)\n",
      "('erb', 1)\n",
      "('der', 1)\n",
      "('nic', 1)\n",
      "('irk', 1)\n",
      "('rge', 1)\n",
      "('yth', 1)\n",
      "('low', 1)\n",
      "('rek', 1)\n",
      "('nch', 1)\n",
      "('ggy', 1)\n",
      "('ely', 1)\n",
      "('rke', 1)\n",
      "('yke', 1)\n",
      "('ylo', 1)\n",
      "('Rem', 1)\n",
      "('ole', 1)\n",
      "('Lay', 1)\n",
      "('old', 1)\n",
      "('Nil', 1)\n",
      "('ele', 1)\n",
      "('vie', 1)\n",
      "('rgo', 1)\n",
      "('win', 1)\n",
      "('Jon', 1)\n",
      "('erl', 1)\n",
      "('ppy', 1)\n",
      "('ith', 1)\n",
      "('Say', 1)\n",
      "('lel', 1)\n",
      "('nus', 1)\n",
      "('lis', 1)\n",
      "('ral', 1)\n",
      "('ris', 1)\n",
      "('owe', 1)\n",
      "('lou', 1)\n",
      "('her', 1)\n",
      "('ric', 1)\n",
      "('hor', 1)\n",
      "('mie', 1)\n",
      "('air', 1)\n",
      "('een', 1)\n",
      "('oth', 1)\n",
      "('had', 1)\n",
      "('Rod', 1)\n",
      "('ish', 1)\n",
      "('igi', 1)\n",
      "('son', 1)\n",
      "('ern', 1)\n",
      "('nah', 1)\n",
      "('arl', 1)\n",
      "('eid', 1)\n",
      "('way', 1)\n",
      "('sie', 1)\n",
      "('jie', 1)\n",
      "('rve', 1)\n",
      "('tes', 1)\n",
      "('any', 1)\n",
      "('rby', 1)\n",
      "('die', 1)\n",
      "('org', 1)\n",
      "('ome', 1)\n",
      "('rol', 1)\n",
      "('tie', 1)\n",
      "('tri', 1)\n",
      "('leb', 1)\n",
      "('oni', 1)\n",
      "('mey', 1)\n",
      "('afa', 1)\n",
      "('mon', 1)\n",
      "('hby', 1)\n",
      "('Bel', 1)\n",
      "('aus', 1)\n",
      "('Aub', 1)\n",
      "('Sid', 1)\n",
      "('Raf', 1)\n",
      "('ott', 1)\n",
      "('mil', 1)\n",
      "('nji', 1)\n",
      "('nid', 1)\n",
      "('dri', 1)\n",
      "('lge', 1)\n",
      "('oel', 1)\n",
      "('Rad', 1)\n",
      "('ray', 1)\n",
      "('rik', 1)\n",
      "('ten', 1)\n",
      "('nly', 1)\n",
      "('uke', 1)\n",
      "('kki', 1)\n",
      "('nce', 1)\n",
      "('gan', 1)\n",
      "('edd', 1)\n",
      "('add', 1)\n",
      "('men', 1)\n",
      "('Sig', 1)\n",
      "('eme', 1)\n",
      "('iko', 1)\n",
      "('kie', 1)\n",
      "('rit', 1)\n",
      "('tum', 1)\n",
      "('lby', 1)\n",
      "('ree', 1)\n",
      "('ner', 1)\n",
      "('cke', 1)\n",
      "('ano', 1)\n",
      "('lce', 1)\n",
      "('iam', 1)\n",
      "('see', 1)\n",
      "('che', 1)\n",
      "('nan', 1)\n",
      "('bin', 1)\n",
      "('hna', 1)\n",
      "('dee', 1)\n",
      "('gsy', 1)\n",
      "('lon', 1)\n",
      "('yan', 1)\n",
      "('mit', 1)\n",
      "('kee', 1)\n"
     ]
    }
   ],
   "source": [
    "endings=[x[-3:] for x in failed_names]\n",
    "\n",
    "end_freq=((key,freq )for (key,freq) in zip(Counter(endings).keys(),Counter(endings).values()))\n",
    "end_freq=sorted(end_freq, key=lambda tup: tup[1],reverse=True)\n",
    "\n",
    "for i in end_freq:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейросеть показала более высокие значения метрик, так как  данные семейства алгоритмов хорошо справляются с поиском закономерностией в данных, Байесовский классификатор же учитывает только корреляцию n-грамм. Однако, разница не так велика, так как для более высоких значений результата работы нейросети объем выборки должен быть больше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
